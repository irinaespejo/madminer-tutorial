---
redirect_from:
  - "/tutorial-particle-physics/3c-likelihood"
interact_link: content/tutorial_particle_physics/3c_likelihood.ipynb
kernel_name: python2
kernel_path: content/tutorial_particle_physics
has_widgets: false
title: |-
  Likelihood
pagenum: 13
prev_page:
  url: /tutorial_particle_physics/3b_score.html
next_page:
  url: /statistics.html
suffix: .ipynb
search: likelihood theta train estimator tutorial part samples events x madminer neural function ratio well training run test data point not class sampleaugmenter our given functions only techniques scandal sample points network c its load different augmented information need quite used machine learning sampling estimators e parameter distribution p selected event second gold inference methods joint saves any defines parameterized evaluation method benchmark hypothesis sm note estimates evaluate free log particle physics johann brehmer felix kling irina espejo kyle cranmer third itself rather assume instead b just filename later preparations unweighted simulations ready take care remaining book keeping steps before unweights

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Likelihood</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="MadMiner-particle-physics-tutorial">MadMiner particle physics tutorial<a class="anchor-link" href="#MadMiner-particle-physics-tutorial"> </a></h1><h1 id="Part-3c:-Training-a-likelihood-estimator">Part 3c: Training a likelihood estimator<a class="anchor-link" href="#Part-3c:-Training-a-likelihood-estimator"> </a></h1><p>Johann Brehmer, Felix Kling, Irina Espejo, and Kyle Cranmer 2018-2019</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In part 3c of this tutorial we will train a third neural estimator: this time of the likelihood function itself (rather than its ratio). We assume that you have run part 1 and 2a of this tutorial. If, instead of 2a, you have run part 2b, you just have to load a different filename later.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Preparations">Preparations<a class="anchor-link" href="#Preparations"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">unicode_literals</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">madminer.sampling</span> <span class="kn">import</span> <span class="n">SampleAugmenter</span>
<span class="kn">from</span> <span class="nn">madminer</span> <span class="kn">import</span> <span class="n">sampling</span>
<span class="kn">from</span> <span class="nn">madminer.ml</span> <span class="kn">import</span> <span class="n">LikelihoodEstimator</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># MadMiner output</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
    <span class="n">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)-5.5s</span><span class="s1"> </span><span class="si">%(name)-20.20s</span><span class="s1"> </span><span class="si">%(levelname)-7.7s</span><span class="s1"> </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span>
    <span class="n">datefmt</span><span class="o">=</span><span class="s1">&#39;%H:%M&#39;</span><span class="p">,</span>
    <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span>
<span class="p">)</span>

<span class="c1"># Output of all other modules (e.g. matplotlib)</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">logging</span><span class="o">.</span><span class="n">Logger</span><span class="o">.</span><span class="n">manager</span><span class="o">.</span><span class="n">loggerDict</span><span class="p">:</span>
    <span class="k">if</span> <span class="s2">&quot;madminer&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-Make-(unweighted)-training-and-test-samples-with-augmented-data">1. Make (unweighted) training and test samples with augmented data<a class="anchor-link" href="#1.-Make-(unweighted)-training-and-test-samples-with-augmented-data"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At this point, we have all the information we need from the simulations. But the data is not quite ready to be used for machine learning. The <code>madminer.sampling</code> class <code>SampleAugmenter</code> will take care of the remaining book-keeping steps before we can train our estimators:</p>
<p>First, it unweights the samples, i.e. for a given parameter vector <code>theta</code> (or a distribution <code>p(theta)</code>) it picks events <code>x</code> such that their distribution follows <code>p(x|theta)</code>. The selected samples will all come from the event file we have so far, but their frequency is changed -- some events will appear multiple times, some will disappear.</p>
<p>Second, <code>SampleAugmenter</code> calculates all the augmented data ("gold") that is the key to our new inference methods. Depending on the specific technique, these are the joint likelihood ratio and / or the joint score. It saves all these pieces of information for the selected events in a set of numpy files that can easily be used in any machine learning framework.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">sampler</span> <span class="o">=</span> <span class="n">SampleAugmenter</span><span class="p">(</span><span class="s1">&#39;data/lhe_data_shuffled.h5&#39;</span><span class="p">)</span>
<span class="c1"># sampler = SampleAugmenter(&#39;data/delphes_data_shuffled.h5&#39;)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>03:42 madminer.analysis    INFO    Loading data from data/lhe_data_shuffled.h5
03:42 madminer.analysis    INFO    Found 2 parameters
03:42 madminer.analysis    INFO    Did not find nuisance parameters
03:42 madminer.analysis    INFO    Found 6 benchmarks, of which 6 physical
03:42 madminer.analysis    INFO    Found 3 observables
03:42 madminer.analysis    INFO    Found 89991 events
03:42 madminer.analysis    INFO      49991 signal events sampled from benchmark sm
03:42 madminer.analysis    INFO      10000 signal events sampled from benchmark w
03:42 madminer.analysis    INFO      10000 signal events sampled from benchmark neg_w
03:42 madminer.analysis    INFO      10000 signal events sampled from benchmark ww
03:42 madminer.analysis    INFO      10000 signal events sampled from benchmark neg_ww
03:42 madminer.analysis    INFO    Found morphing setup with 6 components
03:42 madminer.analysis    INFO    Did not find nuisance morphing setup
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>SampleAugmenter</code> class defines five different high-level functions to generate train or test samples:</p>
<ul>
<li><code>sample_train_plain()</code>, which only saves observations x, for instance for histograms or ABC;</li>
<li><code>sample_train_local()</code> for methods like SALLY and SALLINO, which will be demonstrated in the second part of the tutorial;</li>
<li><code>sample_train_density()</code> for neural density estimation techniques like MAF or SCANDAL;</li>
<li><code>sample_train_ratio()</code> for techniques like CARL, ROLR, CASCAL, and RASCAL, when only theta0 is parameterized;</li>
<li><code>sample_train_more_ratios()</code> for the same techniques, but with both theta0 and theta1 parameterized;</li>
<li><code>sample_test()</code> for the evaluation of any method.</li>
</ul>
<p>For the arguments <code>theta</code>, <code>theta0</code>, or <code>theta1</code>, you can (and should!) use the helper functions <code>benchmark()</code>, <code>benchmarks()</code>, <code>morphing_point()</code>, <code>morphing_points()</code>, and <code>random_morphing_points()</code>, all defined in the <code>madminer.sampling</code> module.</p>
<p>Here we'll train a likelihood estimator with the SCANDAL method, so we focus on the <code>extract_samples_train_density()</code> function. We'll sample the numerator hypothesis in the likelihood ratio with 1000 points drawn from a Gaussian prior, and fix the denominator hypothesis to the SM.</p>
<p>Note the keyword <code>sample_only_from_closest_benchmark=True</code>, which makes sure that for each parameter point we only use the events that were originally (in MG) generated from the closest benchmark. This reduces the statistical fluctuations in the outcome quite a bit.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">t_xz</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">sample_train_density</span><span class="p">(</span>
    <span class="n">theta</span><span class="o">=</span><span class="n">sampling</span><span class="o">.</span><span class="n">random_morphing_points</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">[(</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)]),</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">500000</span><span class="p">,</span>
    <span class="n">folder</span><span class="o">=</span><span class="s1">&#39;./data/samples&#39;</span><span class="p">,</span>
    <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;train_density&#39;</span><span class="p">,</span>
    <span class="n">sample_only_from_closest_benchmark</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>03:42 madminer.sampling    INFO    Extracting training sample for non-local score-based methods. Sampling and score evaluation according to (u&#39;random_morphing_points&#39;, (10000, [(u&#39;gaussian&#39;, 0.0, 0.5), (u&#39;gaussian&#39;, 0.0, 0.5)]))
03:42 madminer.sampling    INFO    Starting sampling serially
03:42 madminer.sampling    WARNING Large statistical uncertainty on the total cross section when sampling from theta = [0.58710176 1.2685223 ]: (0.001242 +/- 0.000441) pb (35.5351101412644 %). Skipping these warnings in the future...
03:43 madminer.sampling    INFO    Sampling from parameter point 500 / 10000
03:43 madminer.sampling    INFO    Sampling from parameter point 1000 / 10000
03:44 madminer.sampling    INFO    Sampling from parameter point 1500 / 10000
03:45 madminer.sampling    INFO    Sampling from parameter point 2000 / 10000
03:45 madminer.sampling    INFO    Sampling from parameter point 2500 / 10000
03:47 madminer.sampling    INFO    Sampling from parameter point 3000 / 10000
03:48 madminer.sampling    INFO    Sampling from parameter point 3500 / 10000
03:48 madminer.sampling    INFO    Sampling from parameter point 4000 / 10000
03:49 madminer.sampling    WARNING For this value of theta, 1 / 29993 events have negative weight and will be ignored
03:49 madminer.sampling    INFO    Sampling from parameter point 4500 / 10000
03:50 madminer.sampling    INFO    Sampling from parameter point 5000 / 10000
03:50 madminer.sampling    INFO    Sampling from parameter point 5500 / 10000
03:51 madminer.sampling    INFO    Sampling from parameter point 6000 / 10000
03:52 madminer.sampling    INFO    Sampling from parameter point 6500 / 10000
03:53 madminer.sampling    INFO    Sampling from parameter point 7000 / 10000
03:53 madminer.sampling    INFO    Sampling from parameter point 7500 / 10000
03:54 madminer.sampling    INFO    Sampling from parameter point 8000 / 10000
04:55 madminer.sampling    INFO    Sampling from parameter point 8500 / 10000
05:16 madminer.sampling    INFO    Sampling from parameter point 9000 / 10000
05:17 madminer.sampling    INFO    Sampling from parameter point 9500 / 10000
05:18 madminer.sampling    INFO    Sampling from parameter point 10000 / 10000
05:18 madminer.sampling    INFO    Effective number of samples: mean 637.1639228954328, with individual thetas ranging from 11.197537298152856 to 28628.63290722551
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the evaluation we'll need a test sample:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">sample_test</span><span class="p">(</span>
    <span class="n">theta</span><span class="o">=</span><span class="n">sampling</span><span class="o">.</span><span class="n">benchmark</span><span class="p">(</span><span class="s1">&#39;sm&#39;</span><span class="p">),</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">folder</span><span class="o">=</span><span class="s1">&#39;./data/samples&#39;</span><span class="p">,</span>
    <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;test&#39;</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>05:18 madminer.sampling    INFO    Extracting evaluation sample. Sampling according to sm
05:18 madminer.sampling    INFO    Starting sampling serially
05:18 madminer.sampling    INFO    Sampling from parameter point 1 / 1
05:18 madminer.sampling    INFO    Effective number of samples: mean 10015.0, with individual thetas ranging from 10015.0 to 10015.0
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Train-likelihood-estimator">2. Train likelihood estimator<a class="anchor-link" href="#2.-Train-likelihood-estimator"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's now time to build the neural network that estimates the likelihood ratio. The central object for this is the <code>madminer.ml.ParameterizedRatioEstimator</code> class. It defines functions that train, save, load, and evaluate the estimators.</p>
<p>In the initialization, the keywords <code>n_hidden</code> and <code>activation</code> define the architecture of the (fully connected) neural network:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">estimator</span> <span class="o">=</span> <span class="n">LikelihoodEstimator</span><span class="p">(</span>
    <span class="n">n_mades</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">n_hidden</span><span class="o">=</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span><span class="mi">60</span><span class="p">),</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To train this model we will minimize the SCANDAL loss function described in <a href="https://arxiv.org/abs/1805.12244">"Mining gold from implicit models to improve likelihood-free inference"</a>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">estimator</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">method</span><span class="o">=</span><span class="s1">&#39;scandal&#39;</span><span class="p">,</span>
    <span class="n">theta</span><span class="o">=</span><span class="s1">&#39;data/samples/theta_train_density.npy&#39;</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;data/samples/x_train_density.npy&#39;</span><span class="p">,</span>
    <span class="n">t_xz</span><span class="o">=</span><span class="s1">&#39;data/samples/t_xz_train_density.npy&#39;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span>
    <span class="n">n_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">initial_lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">estimator</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;models/scandal&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>05:18 madminer.ml          INFO    Starting training
05:18 madminer.ml          INFO      Method:                 scandal
05:18 madminer.ml          INFO      alpha:                  10.0
05:18 madminer.ml          INFO      Batch size:             128
05:18 madminer.ml          INFO      Optimizer:              amsgrad
05:18 madminer.ml          INFO      Epochs:                 10
05:18 madminer.ml          INFO      Learning rate:          0.01 initially, decaying to 0.0001
05:18 madminer.ml          INFO      Validation split:       0.25
05:18 madminer.ml          INFO      Early stopping:         True
05:18 madminer.ml          INFO      Scale inputs:           True
05:18 madminer.ml          INFO      Shuffle labels          False
05:18 madminer.ml          INFO      Samples:                all
05:18 madminer.ml          INFO    Loading training data
05:18 madminer.utils.vario INFO      Loading data/samples/theta_train_density.npy into RAM
05:18 madminer.utils.vario INFO      Loading data/samples/x_train_density.npy into RAM
05:18 madminer.utils.vario INFO      Loading data/samples/t_xz_train_density.npy into RAM
05:18 madminer.ml          INFO    Found 500000 samples with 2 parameters and 3 observables
05:18 madminer.ml          INFO    Setting up input rescaling
05:18 madminer.ml          INFO    Rescaling parameters
05:18 madminer.ml          INFO    Setting up parameter rescaling
05:18 madminer.ml          INFO    Training model
05:18 madminer.utils.ml.tr INFO    Training on CPU with single precision
05:19 madminer.utils.ml.tr INFO      Epoch   1: train loss  4.92008 (nll:  3.866, mse_score:  0.105)
05:19 madminer.utils.ml.tr INFO                 val. loss   4.88054 (nll:  3.877, mse_score:  0.100)
05:20 madminer.utils.ml.tr INFO      Epoch   2: train loss  4.81460 (nll:  3.866, mse_score:  0.095)
05:20 madminer.utils.ml.tr INFO                 val. loss   4.74922 (nll:  3.872, mse_score:  0.088)
05:21 madminer.utils.ml.tr INFO      Epoch   3: train loss  4.70997 (nll:  3.839, mse_score:  0.087)
05:21 madminer.utils.ml.tr INFO                 val. loss   4.68496 (nll:  3.835, mse_score:  0.085)
05:22 madminer.utils.ml.tr INFO      Epoch   4: train loss  4.66875 (nll:  3.823, mse_score:  0.085)
05:22 madminer.utils.ml.tr INFO                 val. loss   4.61774 (nll:  3.826, mse_score:  0.079)
05:23 madminer.utils.ml.tr INFO      Epoch   5: train loss  4.62898 (nll:  3.815, mse_score:  0.081)
05:23 madminer.utils.ml.tr INFO                 val. loss   4.59818 (nll:  3.813, mse_score:  0.078)
05:24 madminer.utils.ml.tr INFO      Epoch   6: train loss  4.60716 (nll:  3.809, mse_score:  0.080)
05:24 madminer.utils.ml.tr INFO                 val. loss   4.59749 (nll:  3.814, mse_score:  0.078)
05:25 madminer.utils.ml.tr INFO      Epoch   7: train loss  4.59577 (nll:  3.806, mse_score:  0.079)
05:25 madminer.utils.ml.tr INFO                 val. loss   4.57310 (nll:  3.807, mse_score:  0.077)
05:26 madminer.utils.ml.tr INFO      Epoch   8: train loss  4.58374 (nll:  3.806, mse_score:  0.078)
05:26 madminer.utils.ml.tr INFO                 val. loss   4.56814 (nll:  3.808, mse_score:  0.076)
05:27 madminer.utils.ml.tr INFO      Epoch   9: train loss  4.58109 (nll:  3.804, mse_score:  0.078)
05:27 madminer.utils.ml.tr INFO                 val. loss   4.56638 (nll:  3.807, mse_score:  0.076)
05:28 madminer.utils.ml.tr INFO      Epoch  10: train loss  4.57864 (nll:  3.804, mse_score:  0.077)
05:28 madminer.utils.ml.tr INFO                 val. loss   4.56464 (nll:  3.806, mse_score:  0.076)
05:28 madminer.utils.ml.tr INFO    Early stopping did not improve performance
05:28 madminer.utils.ml.tr INFO    Training time spend on:
05:28 madminer.utils.ml.tr INFO                      initialize model:   0.00h
05:28 madminer.utils.ml.tr INFO                                   ALL:   0.16h
05:28 madminer.utils.ml.tr INFO                            check data:   0.00h
05:28 madminer.utils.ml.tr INFO                          make dataset:   0.00h
05:28 madminer.utils.ml.tr INFO                       make dataloader:   0.00h
05:28 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h
05:28 madminer.utils.ml.tr INFO                   initialize training:   0.00h
05:28 madminer.utils.ml.tr INFO                                set lr:   0.00h
05:28 madminer.utils.ml.tr INFO                   load training batch:   0.02h
05:28 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h
05:28 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.01h
05:28 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.06h
05:28 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h
05:28 madminer.utils.ml.tr INFO                 training forward pass:   0.06h
05:28 madminer.utils.ml.tr INFO                   training sum losses:   0.00h
05:28 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h
05:28 madminer.utils.ml.tr INFO                         opt: backward:   0.04h
05:28 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h
05:28 madminer.utils.ml.tr INFO                             opt: step:   0.01h
05:28 madminer.utils.ml.tr INFO                        optimizer step:   0.06h
05:28 madminer.utils.ml.tr INFO                 load validation batch:   0.01h
05:28 madminer.utils.ml.tr INFO               validation forward pass:   0.02h
05:28 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h
05:28 madminer.utils.ml.tr INFO                        early stopping:   0.00h
05:28 madminer.utils.ml.tr INFO                          report epoch:   0.00h
05:28 madminer.ml          INFO    Saving model to models/scandal
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>(nll:  3.830, mse_score:  0.064</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.-Evaluate-likelihood-estimator">3. Evaluate likelihood estimator<a class="anchor-link" href="#3.-Evaluate-likelihood-estimator"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>estimator.evaluate_log_likelihood(theta,x)</code> estimated the log likelihood for all combination between the given phase-space points <code>x</code> and parameters <code>theta</code>. That is, if given 100 events <code>x</code> and a grid of 25 <code>theta</code> points, it will return 25*100 estimates for the log likelihood, indexed by <code>[i_theta,i_x]</code>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">theta_each</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mi">25</span><span class="p">)</span>
<span class="n">theta0</span><span class="p">,</span> <span class="n">theta1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">theta_each</span><span class="p">,</span> <span class="n">theta_each</span><span class="p">)</span>
<span class="n">theta_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">theta0</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">theta1</span><span class="o">.</span><span class="n">flatten</span><span class="p">()))</span><span class="o">.</span><span class="n">T</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;data/samples/theta_grid.npy&#39;</span><span class="p">,</span> <span class="n">theta_grid</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">estimator</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;models/scandal&#39;</span><span class="p">)</span>

<span class="n">log_p_hat</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">evaluate_log_likelihood</span><span class="p">(</span>
    <span class="n">theta</span><span class="o">=</span><span class="s1">&#39;data/samples/theta_grid.npy&#39;</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;data/samples/x_test.npy&#39;</span><span class="p">,</span>
    <span class="n">evaluate_score</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>05:28 madminer.ml          INFO    Loading model from models/scandal
05:28 madminer.utils.vario INFO      Loading data/samples/theta_grid.npy into RAM
05:28 madminer.utils.vario INFO      Loading data/samples/x_test.npy into RAM
05:28 madminer.ml          INFO    Starting ratio evaluation for 625000 x-theta combinations
05:28 madminer.ml          INFO    Evaluation done
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's look at the result:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="n">bin_size</span> <span class="o">=</span> <span class="n">theta_each</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">theta_each</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">edges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">theta_each</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">bin_size</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">theta_each</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">bin_size</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">theta_each</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

<span class="n">expected_llr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_p_hat</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">best_fit</span> <span class="o">=</span> <span class="n">theta_grid</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="o">-</span><span class="mf">2.</span><span class="o">*</span><span class="n">expected_llr</span><span class="p">)]</span>

<span class="n">cmin</span><span class="p">,</span> <span class="n">cmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">expected_llr</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">expected_llr</span><span class="p">)</span>
    
<span class="n">pcm</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">expected_llr</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">25</span><span class="p">,</span><span class="mi">25</span><span class="p">)),</span>
                    <span class="n">norm</span><span class="o">=</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">cmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">cmax</span><span class="p">),</span>
                    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis_r&#39;</span><span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">pcm</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">extend</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">best_fit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">best_fit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mf">80.</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_0$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbb{E}_x [ -2\, \log \,\hat{r}(x | \theta, \theta_{SM}) ]$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/tutorial_particle_physics/3c_likelihood_23_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that in this tutorial our sample size was very small, and the network might not really have a chance to converge to the correct likelihood function. So don't worry if you find a minimum that is not at the right point (the SM, i.e. the origin in this plot). Feel free to dial up the event numbers in the run card as well as the training samples and see what happens then!</p>

</div>
</div>
</div>
</div>

 


    </main>
    